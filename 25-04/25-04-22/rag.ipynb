{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537b454e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### DOCX TO JSONL , simple conversion ##################### \n",
    "\n",
    "# token : \n",
    "!pip uninstall -y docx\n",
    "!pip install python-docx\n",
    "\n",
    "from docx import Document\n",
    "import json\n",
    "\n",
    "# Paths for full docx\n",
    "docx_path = \"/content/ug1023-sdaccel-user-guide.docx\"\n",
    "jsonl_path = \"/content/ug1023-sdaccel-user-guide.jsonl\"\n",
    "\n",
    "def convert_docx_to_jsonl(docx_path, jsonl_path):\n",
    "    doc = Document(docx_path)\n",
    "    with open(jsonl_path, \"w\", encoding=\"utf-8\") as jsonl_file:\n",
    "        for i, para in enumerate(doc.paragraphs):\n",
    "            text = para.text.strip()\n",
    "            if text:  # Skip empty paragraphs\n",
    "                record = {\"id\": f\"para_{i}\", \"text\": text}\n",
    "                jsonl_file.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "    return jsonl_path\n",
    "\n",
    "converted_path = convert_docx_to_jsonl(docx_path, jsonl_path)\n",
    "converted_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18350d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### ORIGINAL RAG #####################\n",
    "\n",
    "# 1. Install dependencies\n",
    "!pip install -q transformers datasets sentence-transformers faiss-cpu scikit-learn\n",
    "\n",
    "# 2. Load & chunk your JSONL into passages\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the raw JSONL (each line: {\"id\": ..., \"text\": ...})\n",
    "ds = load_dataset(\"json\", data_files=\"/content/ug1023-sdaccel-user-guide.jsonl\", split=\"train\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "# Chunk long texts into <=512-token passages\n",
    "def chunk_docs(example, max_len=512):\n",
    "    ids, texts = [], []\n",
    "    tok = tokenizer(example[\"text\"], return_attention_mask=False)[\"input_ids\"]\n",
    "    for i in range(0, len(tok), max_len):\n",
    "        chunk = tokenizer.decode(tok[i : i + max_len], skip_special_tokens=True)\n",
    "        ids.append(f\"{example['id']}_c{i}\")\n",
    "        texts.append(chunk)\n",
    "    return {\"id\": ids, \"text\": texts}\n",
    "\n",
    "ds_p = ds.map(chunk_docs, batched=False, remove_columns=ds.column_names).flatten()\n",
    "\n",
    "# 3. Build embeddings + FAISS index\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "passages = [txt if isinstance(txt, str) else \" \".join(txt) for txt in ds_p[\"text\"]]\n",
    "embs = embedder.encode(passages, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "dim = embs.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "index.add(embs)\n",
    "faiss.write_index(index, \"cli_index.faiss\")\n",
    "\n",
    "# 4. Retrieval function\n",
    "def retrieve(question: str, k: int = 5) -> list[str]:\n",
    "    q_emb = embedder.encode([question], normalize_embeddings=True)\n",
    "    _, idxs = index.search(q_emb, k)\n",
    "    return [passages[i] for i in idxs[0]]\n",
    "\n",
    "# 5. Load the BART generator\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "# 6. Retrieval-augmented generation example\n",
    "query = \"What is Local Memory?\"\n",
    "top_k = retrieve(query, k=5)\n",
    "\n",
    "prompt = \"\\n\\n\".join(top_k) + f\"\\n\\nQuestion: {query}\"\n",
    "inputs = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    padding=\"longest\",\n",
    "    max_length=1024  # explicit truncation cutoff\n",
    ")\n",
    "outputs = model.generate(**inputs, max_new_tokens=60, num_beams=4)\n",
    "answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Answer:\\n\", answer)\n",
    "\n",
    "# 7. Retrieval metrics: Precision@k and Recall@k\n",
    "import numpy as np\n",
    "\n",
    "def precision_recall_at_k(retriever_fn, eval_queries, ground_truth, k=5):\n",
    "    precisions, recalls = [], []\n",
    "    for q in eval_queries:\n",
    "        preds = retriever_fn(q, k)\n",
    "        trues = set(ground_truth.get(q, []))\n",
    "        tp = len(set(preds) & trues)\n",
    "        precisions.append(tp / k)\n",
    "        recalls.append(tp / len(trues) if trues else 0.0)\n",
    "    return np.mean(precisions), np.mean(recalls)\n",
    "\n",
    "# Example evaluation data (replace with real labels)\n",
    "eval_queries = [\"What is Local Memory?\", \"Explain OpenCL Memory Model\"]\n",
    "ground_truth = {\n",
    "    \"What is Local Memory?\": [\"para_10_c0\", \"para_10_c1\"],\n",
    "    \"Explain OpenCL Memory Model\": [\"para_12_c0\", \"para_12_c1\"]\n",
    "}\n",
    "\n",
    "prec5, rec5 = precision_recall_at_k(retrieve, eval_queries, ground_truth, k=5)\n",
    "print(f\"Precision@5: {prec5:.3f}, Recall@5: {rec5:.3f}\")\n",
    "\n",
    "# 8. Embedding cluster–quality metrics: intra vs. inter cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import itertools\n",
    "\n",
    "def cluster_similarity(embeddings, clusters):\n",
    "    intra_sims, inter_sims = [], []\n",
    "    for label, idxs in clusters.items():\n",
    "        if len(idxs) < 2:\n",
    "            continue\n",
    "        sims = cosine_similarity(embeddings[idxs], embeddings[idxs])\n",
    "        intra_sims += [sims[i, j] for i, j in itertools.permutations(range(len(idxs)), 2)]\n",
    "    labels = list(clusters)\n",
    "    for a, b in itertools.combinations(labels, 2):\n",
    "        sims = cosine_similarity(embeddings[clusters[a]], embeddings[clusters[b]])\n",
    "        inter_sims += sims.flatten().tolist()\n",
    "    return np.mean(intra_sims), np.mean(inter_sims)\n",
    "\n",
    "# Example clusters (replace with real indices)\n",
    "clusters = {\n",
    "    \"LocalMemory\": [10, 11, 12],\n",
    "    \"GlobalMemory\": [20, 21, 22]\n",
    "}\n",
    "\n",
    "intra, inter = cluster_similarity(embs, clusters)\n",
    "print(f\"Intra-cluster sim: {intra:.3f}, Inter-cluster sim: {inter:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3cbafd",
   "metadata": {},
   "source": [
    "Answer:\n",
    " Local Memory is memory inside of the FPGA. This memory is typically implemented using block RAM elements in the CPU fabric. The block RAM element is typically used to store and transfer data that must be shared by multiple work items within the same compute unit.Local memory is defined as the region\n",
    "Precision@5: 0.000, Recall@5: 0.000\n",
    "Intra-cluster sim: 0.178, Inter-cluster sim: 0.189"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62bdccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### ORIGINAL RAG #####################\n",
    "\n",
    "# 1. Install dependencies\n",
    "!pip install -q transformers datasets sentence-transformers faiss-cpu\n",
    "\n",
    "# 2. Load & chunk JSONL into passages (same as before)\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "ds = load_dataset(\"json\", data_files=\"/content/ug1023-sdaccel-user-guide.jsonl\", split=\"train\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "def chunk_docs(example, max_len=512):\n",
    "    ids, texts = [], []\n",
    "    tok = tokenizer(example[\"text\"], return_attention_mask=False)[\"input_ids\"]\n",
    "    for i in range(0, len(tok), max_len):\n",
    "        chunk = tokenizer.decode(tok[i : i + max_len], skip_special_tokens=True)\n",
    "        ids.append(f\"{example['id']}_c{i}\")\n",
    "        texts.append(chunk)\n",
    "    return {\"id\": ids, \"text\": texts}\n",
    "\n",
    "ds_p = ds.map(chunk_docs, batched=False, remove_columns=ds.column_names).flatten()\n",
    "\n",
    "# 3. Sentence embeddings + FAISS HNSW index\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss, numpy as np\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "passages = [p if isinstance(p, str) else \" \".join(p) for p in ds_p[\"text\"]]\n",
    "embs = embedder.encode(passages, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "# Build HNSW index for sublinear lookup\n",
    "dim = embs.shape[1]\n",
    "index = faiss.IndexHNSWFlat(dim, 32)  # 32 neighbors per node\n",
    "index.hnsw.efConstruction = 200\n",
    "index.hnsw.efSearch = 50\n",
    "index.add(embs)\n",
    "faiss.write_index(index, \"cli_hnsw_index.faiss\")\n",
    "\n",
    "# 4. Efficient retrieve\n",
    "def efficient_retrieve(question: str, k: int = 5):\n",
    "    q_emb = embedder.encode([question], normalize_embeddings=True)\n",
    "    D, I = index.search(q_emb, k)\n",
    "    return [passages[i] for i in I[0]]\n",
    "\n",
    "# 5. Prepare FiD model inputs\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "fid_tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "fid_model     = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "\n",
    "def build_fid_input(query, contexts):\n",
    "    # Prefix “question” for T5; join each context with special separator\n",
    "    joined = \" </s><s> \".join(contexts)\n",
    "    return f\"question: {query} contexts: {joined}\"\n",
    "\n",
    "# 6. Run Efficient RAG\n",
    "query = \"What is Local Memory?\"\n",
    "ctxs  = efficient_retrieve(query, k=8)  # retrieve more for FiD\n",
    "\n",
    "fid_input = build_fid_input(query, ctxs)\n",
    "inputs    = fid_tokenizer(fid_input, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "outputs   = fid_model.generate(**inputs, max_new_tokens=60, num_beams=4)\n",
    "\n",
    "answer = fid_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Answer:\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28386e97",
   "metadata": {},
   "source": [
    "Answer:\n",
    " the region of system memory that is only accessible to the OpenCLTM device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b608912a",
   "metadata": {},
   "source": [
    "Summary - \n",
    "\n",
    "# We’re using two open-source models in this pipeline:\n",
    "\n",
    "# SentenceTransformer “all-MiniLM-L6-v2” for retrieval embeddings.\n",
    "\n",
    "# A compact, 82 MB model optimized for sentence and passage embeddings with high semantic accuracy .\n",
    "\n",
    "# Facebook BART-base (“facebook/bart-base”) as the generation model.\n",
    "\n",
    "# A 139 M-parameter sequence-to-sequence Transformer pretrained on large text corpora, well-suited for summarization, question answering, and in-context generation ."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
