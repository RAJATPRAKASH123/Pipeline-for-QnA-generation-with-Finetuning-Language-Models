{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7944df94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tried FineTuning but this code didn't work due to dependencies issues with gpu.\n",
    "\n",
    "\n",
    "!pip install python-docx tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeca5f17",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers\n",
    "! pip install transformers accelerate peft datasets bitsandbytes\n",
    "\n",
    "\n",
    "# import os\n",
    "# from huggingface_hub import login\n",
    "# from datasets import load_dataset\n",
    "# from transformers import (\n",
    "#     AutoTokenizer,\n",
    "#     AutoModelForCausalLM,\n",
    "#     BitsAndBytesConfig,\n",
    "#     Trainer,\n",
    "#     TrainingArguments\n",
    "# )\n",
    "# from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# # 1. Authenticate\n",
    "# os.environ[\"HF_TOKEN\"] = \"\"\n",
    "# login(token=os.environ[\"HF_TOKEN\"])\n",
    "\n",
    "# # 2. Load dataset\n",
    "# TRAIN_FILE = \"/content/train.jsonl\"\n",
    "# dataset = load_dataset(\"json\", data_files={\"train\": TRAIN_FILE})[\"train\"]\n",
    "\n",
    "# # 3. Tokenizer + Model w/ 4-bit quantization\n",
    "# MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=False, use_auth_token=True)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_compute_dtype=\"float16\",\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "# )\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL_ID,\n",
    "#     device_map=\"auto\",\n",
    "#     quantization_config=bnb_config,\n",
    "#     use_auth_token=True\n",
    "# )\n",
    "\n",
    "# # 4. Attach LoRA\n",
    "# peft_config = LoraConfig(\n",
    "#     task_type=TaskType.CAUSAL_LM,\n",
    "#     inference_mode=False,\n",
    "#     r=8, lora_alpha=32, lora_dropout=0.1,\n",
    "# )\n",
    "# model = get_peft_model(model, peft_config)\n",
    "\n",
    "# # 5. Tokenization helper for fine-tuning\n",
    "# def tokenize_batch(ex):\n",
    "#     txts = [\n",
    "#         p + c + tokenizer.eos_token\n",
    "#         for p, c in zip(ex[\"prompt\"], ex[\"completion\"])\n",
    "#     ]\n",
    "#     batch = tokenizer(\n",
    "#         txts, truncation=True, max_length=512, padding=\"max_length\"\n",
    "#     )\n",
    "#     batch[\"labels\"] = batch[\"input_ids\"].copy()\n",
    "#     return batch\n",
    "\n",
    "# tokenized = dataset.map(\n",
    "#     tokenize_batch,\n",
    "#     batched=True,\n",
    "#     remove_columns=[\"prompt\", \"completion\"]\n",
    "# )\n",
    "\n",
    "# # 6. Trainer setup\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"tiny_llama_finetuned\",\n",
    "#     per_device_train_batch_size=4,\n",
    "#     gradient_accumulation_steps=8,\n",
    "#     num_train_epochs=3,\n",
    "#     learning_rate=2e-4,\n",
    "#     fp16=True,\n",
    "#     optim=\"paged_adamw_32bit\",\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized,\n",
    "#     tokenizer=tokenizer,\n",
    "# )\n",
    "\n",
    "# # 7. Train!\n",
    "# trainer.train()\n",
    "\n",
    "# # 8. Save adapter + tokenizer\n",
    "# model.save_pretrained(\"tiny_llama_finetuned/lora_adapter\")\n",
    "# tokenizer.save_pretrained(\"tiny_llama_finetuned/tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6c6399",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "================================================ERROR=====================================\n",
    "CUDA SETUP: CUDA detection failed! Possible reasons:\n",
    "1. CUDA driver not installed\n",
    "2. CUDA not installed\n",
    "3. You have multiple conflicting CUDA libraries\n",
    "4. Required library not pre-compiled for this bitsandbytes release!\n",
    "CUDA SETUP: If you compiled from source, try again with `make CUDA_VERSION=DETECTED_CUDA_VERSION` for example, `make CUDA_VERSION=113`.\n",
    "CUDA SETUP: The CUDA version for the compile might depend on your conda install. Inspect CUDA version via `conda list | grep cuda`.\n",
    "================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141995cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding LoRA Adapter & Tokenizer\n",
    "\n",
    "'''\n",
    "1. tokenizer/\n",
    "This directory holds all the files your model needs to turn text into tokens (and back). Typical contents include:\n",
    "\n",
    "tokenizer.json\n",
    "A consolidated JSON with all special tokens, merges, and vocabulary data.\n",
    "\n",
    "vocab.json, merges.txt, or vocab.txt\n",
    "Depending on the tokenizer type (BPE, WordPiece, etc.), these map strings ↔ token IDs.\n",
    "\n",
    "tokenizer_config.json\n",
    "Configuration flags (lowercasing, special tokens, truncation/padding defaults).\n",
    "\n",
    "special_tokens_map.json\n",
    "A map of which token IDs correspond to <pad>, <unk>, <eos>, etc.\n",
    "\n",
    "When you later do:\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tiny_llama_cpu_finetuned/tokenizer\")\n",
    "the AutoTokenizer reads exactly these files and reconstructs the same tokenizer you used for training.\n",
    "\n",
    "\n",
    "\n",
    "2. lora_adapter/\n",
    "This directory holds only the LoRA adapter weights that you trained. LoRA (Low-Rank Adapters) injects small additional weight matrices into the base model, so you don’t have to save or re-train the full 1.1 B parameters.\n",
    "\n",
    "Typical contents include:\n",
    "\n",
    "adapter_model.bin or pytorch_model.bin\n",
    "The learned LoRA weights (usually only a few megabytes).\n",
    "\n",
    "adapter_config.json\n",
    "Tells the PEFT library how you configured LoRA (rank = 8, alpha = 32, dropout = 0.1, which layers to apply, etc.).\n",
    "\n",
    "config.json\n",
    "A tiny snippet describing how PEFT should merge these adapters with the base model at inference time.\n",
    "\n",
    "When you load for inference:\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "model = PeftModel.from_pretrained(base, \"tiny_llama_cpu_finetuned/lora_adapter\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
